Execution
=========

Parsl is designed to support arbitrary execution providers (e.g., PCs, clusters, supercomputers) and execution models (e.g., threads, pilot jobs, etc.).
Parsl scripts are independent of execution provider or executor. Instead, the configuration used to run the script tells Parsl how to execute apps on the desired environment.
Parsl provides a high level abstraction, called a *Block*, for describing the resource configuration for a particular app or script.


Execution Providers
-------------------

Execution providers are responsible for managing execution resources. In the simplest case the local computer is used and parallel workload can be forked to individual threads. For larger resources a Local Resource Manager (LRM) is usually used to manage access to resources. For instance, campus clusters and supercomputers generally use LRMs (schedulers) such as Slurm, Torque/PBS, Condor and Cobalt. Clouds, on the other hand, provide API interfaces that allow much more fine-graind composition of an execution environment. Parsl's execution provider abstracts these different resource types and provides a single uniform interface.

Parsl's execution interface is called ``libsubmit`` (`https://github.com/Parsl/libsubmit <https://github.com/Parsl/libsubmit>`_.)--a Python library that abstracts execution environments and provides a common access interface to resources.
Libsubmit defines a simple interface which includes operations such as submission, status, and job management. It currently supports a variety of providers including Amazon Web Services, Azure, and Jetstream clouds as well as Cobolt, Slurm, Torque, and HTCondor LRMs. New execution providers can be easily added by implementing libsubmit's execution provider interface.

Executors
---------

Depending on the execution provider there are a number of ways to then submit workload to that resource. For example, for local execution you could fork threads or use a pilot job manager, for supercomputing resources you may use pilot jobs, various launchers, or even a distributed execution model such as that provided by Swift/T. Parsl supports these models via an *executor*.
Executors represent available compute resources to which tasks can be submitted. An executor initialized with an Execution Provider can dynamically scale with the resources requirements of the workflow.

Parsl currently supports the following executors:

1. **ThreadPoolExecutor**: This executor supports multi-thread execution on local resources.

2. **IPyParallelExecutor**: This executor supports both local and remote execution using a pilot job model. The IPythonParallel controller is deployed locally and IPythonParallel engines are deployed to execution nodes. IPythonParallel then manages the execution of tasks on connected engines.

3. **Swift/TurbineExecutor**: This executor uses the extreme-scale Turbine model to enable distributed task execution across an MPI environment. This executor is typically used on supercomputers.

These executors cover a broad range of execution requirements. As with other Parsl components there is a standard interface (ParslExecutor) that can be implemented to add support for other executors.

Blocks
------

Configuring non-homogeneous resources in a uniform fashion is a challenging aspect of parallel workflow
execution. Parsl uses an abstraction based on resource units called blocks.

A *Block* is a single unit of resources that is obtained from a execution provider.
Within this block are a number of *Nodes*--individual slices of a computing resource.
Parsl can then create *TaskBlocks* within and across (e.g., for MPI jobs) nodes.  A TaskBlock is a
virtual suballocation in which tasks can be launched. Three different examples of block configurations
are shown in the following figures.

1. A single Block comprised of a Node with one TaskBlock :

   .. image:: ../images/N1_T1.png
      :scale: 75%

2. A single Block comprised on a Node with several TaskBlocks. This configuration is
   most suitable for single threaded python/bash applications running with multicore target systems.
   With taskBlocks proportional to the cores available on the system, several apps could execute
   in parallel.

   .. image:: ../images/N1_T4.png
       :scale: 75%

3. A Block comprised of several Nodes and several TaskBlocks. This configuration
   is generally used only by MPI applications and require support from specific
   MPI launchers supported by the target system such as aprun, srun, mpirun, mpiexec etc.

   .. image:: ../images/N4_T2.png


.. _label-elasticity:

Elasticity
----------


.. note::
   This feature is available since Parsl 0.4.0

As a workflow dag is processed by Parsl, new tasks are added and completed
asynchronously. Parsl interfaces executors with execution providers to construct
scalable execution sites to handle the variable work-load generated by the
workflow. This component is responsible for periodically checking outstanding
tasks and available compute capacity and trigger scaling events to match
workflow needs.

Here's a diagram of a site. A site consists of blocks, which are usually
created by single requests to a Local Resource Manager (LRM) such as slurm,
condor, torque, or even AWS API. The blocks could contain several task blocks
which are separate instances on workers.

.. image:: parsl_scaling.gif

The general shape and bounds of a site are user specified through:

1. ``minBlocks``: Minimum # of blocks to maintain per site
2. ``initBlocks``: # of blocks to provision at initialization of workflow
3. ``maxBlocks``: Maximum # of blocks that can be active at a site from one workflow.


Parallelism
^^^^^^^^^^^

In the context of execution in parsl, the level of parallelism is expressed in terms
of the ratio of taskBlocks to all active tasks. Each taskBlock is capable of executing
a single task at any given time. If there are N taskBlocks for N active tasks, the
full parallelism is expressed (Ratio of 1). If there is one taskBlock for N active tasks,
the least parallelism is expressed and all active tasks are executed sequentially.

.. code:: python

   total_taskBlocks = # of blocks * taskBlocks

   active_tasks = pending_tasks + running_tasks

   Parallelism = total_taskBlocks / active_tasks
               = [0, 1] (i.e,  0 <= p <= 1)

For eg:

| When p = 0,
|         => compute with the least resources possible.
|         infinite tasks are stacked per slot.

.. code:: python
    blocks =  minBlocks           { if active_tasks = 0
              max(minBlocks, 1)   {  else

| When p = 1,
|        => compute with the most resources.
|           one task is stacked per slot.

.. code-block:: python

     blocks = min ( maxBlocks,
                   ceil( active_tasks / slots ) )

| When p = 1/2,
|        => We stack upto 2 tasks per slot before we overflow
|           and request a new block


| let's say min:init:max = 0:0:4 and taskBlocks=2
|


Here is a typical configuration :

.. code:: python

    localIPP = {
        "sites": [
            {"site": "Local_IPP",
             "auth": {
                 "channel": None,
             },
             "execution": {
                 "executor": "ipp",
                 "provider": "local", # Run locally
                 "block": {  # Definition of a block
                     "minBlocks" : 1, # }
                     "maxBlocks" : 2, # }<---- Shape of the blocks
                     "initBlocks": 1, # }
                     "taskBlocks": 4, # <----- No. of workers in a block
                     "parallelism" : 0.5 # <-- Parallelism
                 }
             }
            }]
    }

A diagram that describes the behavior of such a site is below:
In the diagram, X <- task
at 2 tasks :
at 5 tasks, we overflow as the capacity of a single block is fully used.

.. image:: parsl_parallelism.gif


Multi-Site
----------

.. note::
   This feature is available since Parsl 0.4.0

Parsl supports the definition of any number of execution sites in the config,
as well as specifying which of these sites could execute specific apps.

The common scenarios for this feature are:

* The workflow has an initial simulation stage that runs on the compute heavy
  nodes of an HPC system followed by an analysis and visualization stage that
  is suited for the GPU nodes.
* The workflow follows a repeated fan-out, fan-in model where the long running
  fan-out tasks are computed on a cluster and the quick fan-in computation is
  suited for the running on threads on the login node.
* Have apps that wait and evaluate the results of a computation to determine
  whether, the app should be relaunched. Only apps running on threads may
  launch apps. Often, science simulations have stochastic behavior and
  terminate early with a checkpoint. In such cases, having a wrapper app
  that checks for the exit conditions and determines success is ideal.


Here's a code snippet that shows how sites can be specified in the ``App`` decorator.

.. code-block:: python

     #(CPU Heavy app) (CPU Heavy app) (CPU Heavy app) <--- Run on compute queue
     #      |                |               |
     #    (data)           (data)          (data)
     #       \               |              /
     #       (Analysis & Visualization phase)         <--- Run on GPU node

     # A mock Molecular Dynamics simulation app
     @App('bash', dfk, sites=["Theta.Phi"])
     def MD_Sim(arg, outputs=[]):
         return "MD_simulate {} -o {}".format(arg, outputs[0])

     # Visualize results from the mock MD simulation app
     @App('bash', dfk, sites=["Cooley.GPU"])
     def Visualize(inputs=[], outputs=[]):
         bash_array = " ".join(inputs)
         return "viz {} -o {}".format(bash_array, outputs[0])
